{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ¿Qué es Web Scraping?\n",
    "\n",
    "Web scraping es el proceso de extraer datos de sitios web de manera automatizada. Se utilizan scripts o programas para acceder al HTML de una página y extraer información relevante, como texto, imágenes, enlaces, etc.\n",
    "\n",
    "Aplicaciones comunes:\n",
    "\n",
    "- Recolección de datos de productos en tiendas en línea.\n",
    "- Análisis de noticias y artículos.\n",
    "- Seguimiento de precios.\n",
    "- Extracción de información de tablas y estadísticas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prueba en Python con Dos Librerías Diferentes\n",
    "Librerías a usar para scraping:\n",
    "\n",
    "- BeautifulSoup (bs4) – Para analizar y extraer datos de HTML/XML de forma sencilla.\n",
    "- Scrapy – Framework avanzado y robusto para scraping a gran escala."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Realizar una Prueba de Web Scraping\n",
    "\n",
    "## Objetivo:\n",
    "\n",
    "- Extraer títulos, enlaces o cualquier información visible de la página principal de K-Pop en Amino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalacion  BeautifulSoup (bs4)\n",
    "\n",
    "pip install beautifulsoup4 scrapy requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Interests...Times Infinity\n",
      "Go Deep, Geek Out\n",
      "Go Deep, Geek Out\n",
      "Unite with Your People\n",
      "Build Your Own Community\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://aminoapps.com/'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Extraer todos los títulos <h1>\n",
    "titles = soup.find_all('h1')\n",
    "for title in titles:\n",
    "    print(title.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalacion Scrapy\n",
    "pip install scrapy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la terminal, ejecutar:\n",
    "\n",
    "- scrapy startproject scraping_project\n",
    "\n",
    "Esto creará una carpeta llamada scraping_project con la estructura base de Scrapy.\n",
    "Moverse al Proyecto\n",
    "\n",
    "- cd scraping_project\n",
    "Generar un Spider (Rastreador)\n",
    "\n",
    "Usa este comando para crear un spider llamado example_spider que rastrará el sitio de Kpop.\n",
    "\n",
    "- scrapy genspider example_spider https://aminoapps.com/\n",
    "Esto creará un archivo en scraping_project/spiders/ llamado example_spider.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abrir y Editar el Spider :\n",
    " \n",
    " -Spider sin modificar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class ExampleSpiderSpider(scrapy.Spider):\n",
    "    name = \"example_spider\"\n",
    "    allowed_domains = [\"aminoapps.com\"]\n",
    "    start_urls = [\"https://aminoapps.com/c/k-pop-es/home/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Spider Modificado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class ExampleSpiderSpider(scrapy.Spider):\n",
    "    name = \"example_spider\"\n",
    "    allowed_domains = [\"aminoapps.com\"]\n",
    "    start_urls = [\"https://aminoapps.com/c/k-pop-es/home/\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Extraer títulos de publicaciones o secciones\n",
    "        for post in response.css('h2::text'):\n",
    "            yield {\n",
    "                'title': post.get()\n",
    "            }\n",
    "\n",
    "        # Extraer enlaces a otras páginas\n",
    "        for link in response.css('a::attr(href)'):\n",
    "            yield {\n",
    "                'link': response.urljoin(link.get())\n",
    "            }\n",
    "\n",
    "        # Opción de seguir paginación (si existe)\n",
    "        next_page = response.css('a.next::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probar el Spider:\n",
    "Guarda el archivo y vuelve a la terminal.\n",
    "\n",
    "Ejecuta el spider:\n",
    "\n",
    "-scrapy crawl example_spider\n",
    "Ver los resultados:\n",
    "Si deseas guardar los resultados en un archivo JSON o CSV:\n",
    "\n",
    "\n",
    "-scrapy crawl example_spider -o resultados.json\n",
    "o\n",
    "-scrapy crawl example_spider -o resultados.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
